{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aneelabashir786/NLP/blob/main/22f8816_22f3414_NLP_A1_7D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading\n"
      ],
      "metadata": {
        "id": "NP9Abu3wq5dC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCU1LbJZObdA",
        "outputId": "dbb2b51c-c459-426f-cdd1-e17b59c58c6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted folders: ['dataset', '__MACOSX']\n"
          ]
        }
      ],
      "source": [
        "import zipfile, os\n",
        "\n",
        "# Unzip dataset.zip into a folder named \"data/dataset\"\n",
        "zip_path = \"dataset.zip\"          # your zip file is at the root\n",
        "extract_path = \"data/dataset\"     # create this folder\n",
        "\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"Extracted folders:\", os.listdir(extract_path))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile(\"dataset.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"data\")\n"
      ],
      "metadata": {
        "id": "ZL1TpNGxOoGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mapping Urdu (source) -> Roman Urdu ( Target )"
      ],
      "metadata": {
        "id": "a1Z3SFTnrH-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def load_dataset(dataset_path=\"data\"):\n",
        "    urdu_sentences = []\n",
        "    roman_urdu_sentences = []\n",
        "\n",
        "    # Walk through poet folders\n",
        "    for poet in os.listdir(dataset_path):\n",
        "        poet_path = os.path.join(dataset_path, poet)\n",
        "        if not os.path.isdir(poet_path):\n",
        "            continue\n",
        "\n",
        "        ur_path = os.path.join(poet_path, \"ur\")\n",
        "        en_path = os.path.join(poet_path, \"en\")\n",
        "\n",
        "        # Skip poets that don't have both ur & en\n",
        "        if not (os.path.isdir(ur_path) and os.path.isdir(en_path)):\n",
        "            continue\n",
        "\n",
        "        # Match files by name\n",
        "        ur_files = sorted(os.listdir(ur_path))\n",
        "        en_files = sorted(os.listdir(en_path))\n",
        "\n",
        "        # Keep only common files\n",
        "        common_files = set(ur_files).intersection(set(en_files))\n",
        "\n",
        "        for fname in common_files:\n",
        "            ur_file_path = os.path.join(ur_path, fname)\n",
        "            en_file_path = os.path.join(en_path, fname)\n",
        "\n",
        "            with open(ur_file_path, \"r\", encoding=\"utf-8\") as f_ur, \\\n",
        "                 open(en_file_path, \"r\", encoding=\"utf-8\") as f_en:\n",
        "\n",
        "                ur_lines = f_ur.readlines()\n",
        "                en_lines = f_en.readlines()\n",
        "\n",
        "                # Pair line by line\n",
        "                for ur, en in zip(ur_lines, en_lines):\n",
        "                    ur = ur.strip()\n",
        "                    en = en.strip()\n",
        "                    if ur and en:  # skip empty lines\n",
        "                        urdu_sentences.append(ur)\n",
        "                        roman_urdu_sentences.append(en)\n",
        "\n",
        "    print(f\"Loaded {len(urdu_sentences)} sentence pairs.\")\n",
        "    return urdu_sentences, roman_urdu_sentences\n",
        "\n",
        "\n",
        "# Example usage\n",
        "urdu, roman = load_dataset(\"data/dataset\")\n",
        "print(\"Sample Urdu:\", urdu[:5])\n",
        "print(\"Sample Roman Urdu:\", roman[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gi_296trO7ti",
        "outputId": "64233402-37d0-4ec9-b21f-6130516341db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 21003 sentence pairs.\n",
            "Sample Urdu: ['غمزہ نہیں ہوتا کہ اشارا نہیں ہوتا', 'آنکھ ان سے جو ملتی ہے تو کیا کیا نہیں ہوتا', 'جلوہ نہ ہو معنی کا تو صورت کا اثر کیا', 'بلبل گل تصویر کا شیدا نہیں ہوتا', 'اللہ بچائے مرض عشق سے دل کو']\n",
            "Sample Roman Urdu: ['ġhamza nahīñ hotā ki ishārā nahīñ hotā', 'aañkh un se jo miltī hai to kyā kyā nahīñ hotā', 'jalva na ho ma.anī kā to sūrat kā asar kyā', 'bulbul gul-e-tasvīr kā shaidā nahīñ hotā', 'allāh bachā.e maraz-e-ishq se dil ko']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "xCRUwJVg_Ctv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re, unicodedata\n",
        "\n",
        "# --- Put this where your cleaning funcs are (above save_corpus) ---\n",
        "# Extra-strong Urdu normalization & cleaning\n",
        "\n",
        "# 1) Canonical Unicode normalization\n",
        "def _nfc(s: str) -> str:\n",
        "    return unicodedata.normalize(\"NFC\", s)\n",
        "\n",
        "# 2) Common letter unifications (expand as needed)\n",
        "_URDU_MAP = {\n",
        "    # Yeh/Kaf/Heh variants\n",
        "    \"ي\": \"ی\", \"ى\": \"ی\", \"ئ\": \"ی\", \"یٰ\": \"ی\",\n",
        "    \"ك\": \"ک\",\n",
        "    \"ھ\": \"ہ\", \"ۀ\": \"ہ\",\n",
        "    \"ة\": \"ہ\",  # taa marbuta → heh (common in loanwords)\n",
        "    \"ؤ\": \"و\",  # optional: hamza-on-waw → waw\n",
        "    \"أ\": \"ا\", \"إ\": \"ا\", \"آ\": \"ا\",  # alef variants → alef\n",
        "    \"ٱ\": \"ا\",\n",
        "    \"ۃ\": \"ہ\",\n",
        "\n",
        "    # Heh-doachashmee handling (keep as \"ہ\" for simplicity)\n",
        "    \"ہٰ\": \"ہ\",\n",
        "}\n",
        "\n",
        "# 3) Marks to remove: tashkeel/diacritics, tatweel, etc.\n",
        "_DIACRITICS_RE = re.compile(r\"[\\u064B-\\u0652\\u0670\\u0653-\\u065F\\u06D6-\\u06ED]\")  # fathatan..sukun, superscript alef, Quranic marks\n",
        "_TATWEEL_RE   = re.compile(r\"\\u0640\")  # tatweel\n",
        "\n",
        "# 4) Allowed characters: Urdu block + space\n",
        "_URDU_KEEP_RE = re.compile(r\"[^\\u0600-\\u06FF\\s]\")\n",
        "\n",
        "def clean_urdu_text(text: str) -> str:\n",
        "    # Canonical normalize\n",
        "    text = _nfc(text)\n",
        "\n",
        "    # Map common variants\n",
        "    for src, dst in _URDU_MAP.items():\n",
        "        text = text.replace(src, dst)\n",
        "\n",
        "    # Remove diacritics & tatweel\n",
        "    text = _DIACRITICS_RE.sub(\"\", text)\n",
        "    text = _TATWEEL_RE.sub(\"\", text)\n",
        "\n",
        "    # Keep only Urdu letters + space\n",
        "    text = _URDU_KEEP_RE.sub(\"\", text)\n",
        "\n",
        "    # Collapse spaces\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "Wx_Nr07jPBLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzL4FcrDPGrE",
        "outputId": "7474f28e-96e2-4b2c-cb67-cdc26108dd34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "\n",
        "# Train SentencePiece Models\n",
        "\n",
        "def train_tokenizers(vocab_size=8000):\n",
        "    spm.SentencePieceTrainer.Train(\n",
        "        input=\"all_urdu.txt\",\n",
        "        model_prefix=\"urdu_spm\",\n",
        "        vocab_size=vocab_size,\n",
        "        character_coverage=0.9995,\n",
        "        model_type=\"bpe\",\n",
        "        pad_id=0,\n",
        "        bos_id=1,\n",
        "        eos_id=2,\n",
        "        unk_id=3\n",
        "    )\n",
        "    spm.SentencePieceTrainer.Train(\n",
        "        input=\"all_roman.txt\",\n",
        "        model_prefix=\"roman_spm\",\n",
        "        vocab_size=vocab_size,\n",
        "        character_coverage=1.0,\n",
        "        model_type=\"bpe\",\n",
        "        pad_id=0,\n",
        "        bos_id=1,\n",
        "        eos_id=2,\n",
        "        unk_id=3\n",
        "    )\n",
        "\n",
        "# Load Tokenizers\n",
        "\n",
        "def load_tokenizers():\n",
        "    urdu_sp = spm.SentencePieceProcessor()\n",
        "    urdu_sp.load(\"urdu_spm.model\")\n",
        "    roman_sp = spm.SentencePieceProcessor()\n",
        "    roman_sp.load(\"roman_spm.model\")\n",
        "    return urdu_sp, roman_sp\n",
        "\n",
        "\n",
        "\n",
        "# Encode Sentences\n",
        "\n",
        "\n",
        "\n",
        "def encode_sentences(texts, sp, max_len=50):\n",
        "    bos, eos, pad = sp.bos_id(), sp.eos_id(), sp.pad_id()\n",
        "    out = []\n",
        "    for s in texts:\n",
        "        if not isinstance(s, str):\n",
        "            s = str(s)\n",
        "        ids = sp.encode(s, out_type=int)          # ← returns list[int]\n",
        "        ids = [bos] + ids[:max_len-2] + [eos]     # keep room for BOS/EOS\n",
        "        if len(ids) < max_len:\n",
        "            ids += [pad] * (max_len - len(ids))\n",
        "        else:\n",
        "            # ensure last token is EOS after trim\n",
        "            ids[-1] = eos\n",
        "        out.append(ids)\n",
        "    return torch.tensor(out, dtype=torch.long)\n"
      ],
      "metadata": {
        "id": "1BcWzHMsPKOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== MASTER SETUP (run this after any Colab restart / GPU switch) ====\n",
        "!pip -q install sentencepiece\n",
        "\n",
        "import os, re, unicodedata\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sentencepiece as spm\n",
        "\n",
        "# ---------- Cleaning ----------\n",
        "def clean_urdu_text(text):\n",
        "    text = unicodedata.normalize(\"NFC\", text)\n",
        "    repl = {\n",
        "        \"ي\":\"ی\",\"ى\":\"ی\",\"ئ\":\"ی\",\"یٰ\":\"ی\",\n",
        "        \"ك\":\"ک\",\n",
        "        \"ھ\":\"ہ\",\"ۀ\":\"ہ\",\"ة\":\"ہ\",\"ۃ\":\"ہ\",\n",
        "        \"ؤ\":\"و\",\n",
        "        \"أ\":\"ا\",\"إ\":\"ا\",\"آ\":\"ا\",\"ٱ\":\"ا\",\n",
        "        \"ہٰ\":\"ہ\",\n",
        "    }\n",
        "    for s,d in repl.items(): text = text.replace(s,d)\n",
        "    text = re.sub(r\"[\\u064B-\\u0652\\u0670\\u0653-\\u065F\\u06D6-\\u06ED]\", \"\", text)  # diacritics\n",
        "    text = re.sub(r\"\\u0640\", \"\", text)                                            # tatweel\n",
        "    text = re.sub(r\"[^\\u0600-\\u06FF\\s]\", \"\", text)                               # Urdu-only\n",
        "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "def clean_roman_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
        "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "def save_corpus(urdu_lines, roman_lines):\n",
        "    with open(\"all_urdu.txt\",\"w\",encoding=\"utf-8\") as f:\n",
        "        for s in urdu_lines: f.write(clean_urdu_text(s)+\"\\n\")\n",
        "    with open(\"all_roman.txt\",\"w\",encoding=\"utf-8\") as f:\n",
        "        for s in roman_lines: f.write(clean_roman_text(s)+\"\\n\")\n",
        "\n",
        "# ---------- SentencePiece train/load ----------\n",
        "def train_tokenizers(vocab_size=8000):\n",
        "    def _train_with_backoff(input_path, prefix, target_vs):\n",
        "        vs = target_vs\n",
        "        while vs >= 2000:  # don’t go too tiny\n",
        "            try:\n",
        "                spm.SentencePieceTrainer.train(\n",
        "                    input=input_path, model_prefix=prefix, vocab_size=vs,\n",
        "                    pad_id=0, bos_id=1, eos_id=2, unk_id=3, character_coverage=1.0\n",
        "                )\n",
        "                print(f\"[OK] Trained {prefix} with vocab_size={vs}\")\n",
        "                return vs\n",
        "            except RuntimeError as e:\n",
        "                msg = str(e)\n",
        "                if \"Vocabulary size too high\" in msg:\n",
        "                    # back off by 10% (round to int)\n",
        "                    new_vs = max(int(vs * 0.9), vs - 500)\n",
        "                    print(f\"[Retry] {prefix}: {msg.strip()} → trying vocab_size={new_vs}\")\n",
        "                    vs = new_vs\n",
        "                else:\n",
        "                    raise\n",
        "        raise RuntimeError(f\"Failed to train {prefix}: corpus too small.\")\n",
        "\n",
        "    # remove existing models if partially created\n",
        "    for f in [\"urdu.model\",\"urdu.vocab\",\"roman.model\",\"roman.vocab\"]:\n",
        "        if os.path.exists(f): os.remove(f)\n",
        "\n",
        "    ur_vs = _train_with_backoff(\"all_urdu.txt\",  \"urdu\",  vocab_size)\n",
        "    ro_vs = _train_with_backoff(\"all_roman.txt\", \"roman\", vocab_size)\n",
        "    return ur_vs, ro_vs\n",
        "\n",
        "def load_tokenizers():\n",
        "    ur_sp = spm.SentencePieceProcessor(model_file=\"urdu.model\")\n",
        "    ro_sp = spm.SentencePieceProcessor(model_file=\"roman.model\")\n",
        "    return ur_sp, ro_sp\n",
        "\n",
        "# ---------- Encoding (signature matches your current calls) ----------\n",
        "# NOTE: you are calling encode_sentences( sp, texts, max_len )\n",
        "import torch\n",
        "def encode_sentences(sp, texts, max_len=50):\n",
        "    bos, eos, pad = sp.bos_id(), sp.eos_id(), sp.pad_id()\n",
        "    out = []\n",
        "    for s in texts:\n",
        "        s = s if isinstance(s,str) else str(s)\n",
        "        ids = sp.encode(s, out_type=int)\n",
        "        ids = [bos] + ids[:max_len-2] + [eos]\n",
        "        if len(ids) < max_len:\n",
        "            ids += [pad]*(max_len - len(ids))\n",
        "        else:\n",
        "            ids[-1] = eos\n",
        "        out.append(ids)\n",
        "    return torch.tensor(out, dtype=torch.long)\n",
        "\n",
        "# ---------- Main preprocessing pipeline ----------\n",
        "def preprocess_pipeline(urdu, roman, vocab_size=8000, max_len=50):\n",
        "    # clean\n",
        "    urdu_clean  = [clean_urdu_text(s) for s in urdu]\n",
        "    roman_clean = [clean_roman_text(s) for s in roman]\n",
        "    # save\n",
        "    save_corpus(urdu_clean, roman_clean)\n",
        "    # tokenizers\n",
        "    train_tokenizers(vocab_size=vocab_size)\n",
        "    urdu_sp, roman_sp = load_tokenizers()\n",
        "    # splits (50/25/25)\n",
        "    train_ur, temp_ur, train_ro, temp_ro = train_test_split(urdu_clean, roman_clean, test_size=0.5, random_state=42)\n",
        "    val_ur,   test_ur,  val_ro,  test_ro = train_test_split(temp_ur,  temp_ro,  test_size=0.5, random_state=42)\n",
        "    # encode\n",
        "    train_X = encode_sentences(urdu_sp,  train_ur, max_len)\n",
        "    train_Y = encode_sentences(roman_sp, train_ro, max_len)\n",
        "    val_X   = encode_sentences(urdu_sp,  val_ur,   max_len)\n",
        "    val_Y   = encode_sentences(roman_sp, val_ro,   max_len)\n",
        "    test_X  = encode_sentences(urdu_sp,  test_ur,  max_len)\n",
        "    test_Y  = encode_sentences(roman_sp, test_ro,  max_len)\n",
        "    return (train_X, train_Y), (val_X, val_Y), (test_X, test_Y), urdu_sp, roman_sp\n"
      ],
      "metadata": {
        "id": "b8_B3FlwPQF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prereq: you must already have python lists `urdu` and `roman` in memory (aligned).\n",
        "(train_X, train_Y), (val_X, val_Y), (test_X, test_Y), ur_sp, ro_sp = preprocess_pipeline(\n",
        "    urdu, roman, vocab_size=8000, max_len=50\n",
        ")\n",
        "\n",
        "print(\"Train shapes:\", train_X.shape, train_Y.shape)\n",
        "print(\"Validation shapes:\", val_X.shape, val_Y.shape)\n",
        "print(\"Test shapes:\", test_X.shape, test_Y.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-re1xcmcPUtv",
        "outputId": "713e36ca-a2d6-46f2-ed4a-fdf36aa4b40e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Retry] urdu: Internal: src/trainer_interface.cc(664) [(trainer_spec_.vocab_size()) == (model_proto->pieces_size())] Vocabulary size too high (8000). Please set it to a value <= 5927. → trying vocab_size=7500\n",
            "[Retry] urdu: Internal: src/trainer_interface.cc(664) [(trainer_spec_.vocab_size()) == (model_proto->pieces_size())] Vocabulary size too high (7500). Please set it to a value <= 5927. → trying vocab_size=7000\n",
            "[Retry] urdu: Internal: src/trainer_interface.cc(664) [(trainer_spec_.vocab_size()) == (model_proto->pieces_size())] Vocabulary size too high (7000). Please set it to a value <= 5927. → trying vocab_size=6500\n",
            "[Retry] urdu: Internal: src/trainer_interface.cc(664) [(trainer_spec_.vocab_size()) == (model_proto->pieces_size())] Vocabulary size too high (6500). Please set it to a value <= 5927. → trying vocab_size=6000\n",
            "[Retry] urdu: Internal: src/trainer_interface.cc(664) [(trainer_spec_.vocab_size()) == (model_proto->pieces_size())] Vocabulary size too high (6000). Please set it to a value <= 5927. → trying vocab_size=5500\n",
            "[OK] Trained urdu with vocab_size=5500\n",
            "[OK] Trained roman with vocab_size=8000\n",
            "Train shapes: torch.Size([10501, 50]) torch.Size([10501, 50])\n",
            "Validation shapes: torch.Size([5251, 50]) torch.Size([5251, 50])\n",
            "Test shapes: torch.Size([5251, 50]) torch.Size([5251, 50])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- Preprocessing Verification ----------------\n",
        "\n",
        "print(\"=== Vocabulary & Special Tokens ===\")\n",
        "print(\"Urdu vocab size:\", ur_sp.get_piece_size())\n",
        "print(\"Roman vocab size:\", ro_sp.get_piece_size())\n",
        "print(\"Special tokens Urdu:\", ur_sp.pad_id(), ur_sp.bos_id(), ur_sp.eos_id())\n",
        "print(\"Special tokens Roman:\", ro_sp.pad_id(), ro_sp.bos_id(), ro_sp.eos_id())\n",
        "assert ur_sp.pad_id()==0 and ur_sp.bos_id()==1 and ur_sp.eos_id()==2\n",
        "assert ro_sp.pad_id()==0 and ro_sp.bos_id()==1 and ro_sp.eos_id()==2\n",
        "\n",
        "print(\"\\n=== Encode / Decode Sanity ===\")\n",
        "urdu_sample = \"مجھے اردو بہت پسند ہے\"\n",
        "roman_sample = \"mujhe urdu bohot pasand hai\"\n",
        "\n",
        "u_ids = ur_sp.encode(urdu_sample, out_type=int)\n",
        "r_ids = ro_sp.encode(roman_sample, out_type=int)\n",
        "print(\"Urdu sample:\", urdu_sample)\n",
        "print(\"Encoded:\", u_ids)\n",
        "print(\"Decoded:\", ur_sp.decode(u_ids))\n",
        "\n",
        "print(\"Roman sample:\", roman_sample)\n",
        "print(\"Encoded:\", r_ids)\n",
        "print(\"Decoded:\", ro_sp.decode(r_ids))\n",
        "\n",
        "print(\"\\n=== With BOS/EOS & Padding ===\")\n",
        "u_ids_pad = encode_sentences(ur_sp, [urdu_sample], max_len=12)[0]\n",
        "r_ids_pad = encode_sentences(ro_sp, [roman_sample], max_len=12)[0]\n",
        "\n",
        "\n",
        "# convert tensors to Python lists before decoding\n",
        "u_list = u_ids_pad.tolist()\n",
        "r_list = r_ids_pad.tolist()\n",
        "\n",
        "print(\"Urdu padded IDs:\", u_list, \"→\", ur_sp.decode(u_list))\n",
        "print(\"Roman padded IDs:\", r_list, \"→\", ro_sp.decode(r_list))\n",
        "\n",
        "assert u_list[0] == ur_sp.bos_id() and u_list[-1] in [ur_sp.eos_id(), ur_sp.pad_id()]\n",
        "assert r_list[0] == ro_sp.bos_id() and r_list[-1] in [ro_sp.eos_id(), ro_sp.pad_id()]\n",
        "\n",
        "print(\"\\n=== Train/Val/Test Split Check ===\")\n",
        "print(\"Train size:\", len(train_X), \"Val size:\", len(val_X), \"Test size:\", len(test_X))\n",
        "print(\"Example pair:\")\n",
        "print(\"Urdu:\", ur_sp.decode(train_X[0].tolist()))\n",
        "print(\"Roman:\", ro_sp.decode(train_Y[0].tolist()))\n",
        "\n",
        "print(\"\\n Preprocessing verified successfully if no assertion errors.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILt39xiqPYwL",
        "outputId": "94520206-4ffc-48e6-9dcb-1d517da874f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Vocabulary & Special Tokens ===\n",
            "Urdu vocab size: 5500\n",
            "Roman vocab size: 8000\n",
            "Special tokens Urdu: 0 1 2\n",
            "Special tokens Roman: 0 1 2\n",
            "\n",
            "=== Encode / Decode Sanity ===\n",
            "Urdu sample: مجھے اردو بہت پسند ہے\n",
            "Encoded: [1925, 3, 7, 33, 3998, 80, 1324, 4]\n",
            "Decoded: مج ⁇ ے اردو بہت پسند ہے\n",
            "Roman sample: mujhe urdu bohot pasand hai\n",
            "Encoded: [53, 3802, 1094, 6098, 17, 70, 2811, 4]\n",
            "Decoded: mujhe urdu bohot pasand hai\n",
            "\n",
            "=== With BOS/EOS & Padding ===\n",
            "Urdu padded IDs: [1, 1925, 3, 7, 33, 3998, 80, 1324, 4, 2, 0, 0] → مج ⁇ ے اردو بہت پسند ہے\n",
            "Roman padded IDs: [1, 53, 3802, 1094, 6098, 17, 70, 2811, 4, 2, 0, 0] → mujhe urdu bohot pasand hai\n",
            "\n",
            "=== Train/Val/Test Split Check ===\n",
            "Train size: 10501 Val size: 5251 Test size: 5251\n",
            "Example pair:\n",
            "Urdu: عالم ہے فقط مومن جانباز کی میراث\n",
            "Roman: aalam hai faqat mominejbz k mrs\n",
            "\n",
            " Preprocessing verified successfully if no assertion errors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Archetecture\n"
      ],
      "metadata": {
        "id": "lwrbepZ_rWDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# ================ Encoder: 2-layer BiLSTM ================\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim=256, hid_dim=512, n_layers=2, dropout=0.3, pad_idx=0):\n",
        "        super().__init__()\n",
        "        self.embed   = nn.Embedding(input_dim, emb_dim, padding_idx=pad_idx)\n",
        "        self.lstm    = nn.LSTM(emb_dim, hid_dim, num_layers=n_layers,\n",
        "                               bidirectional=True, batch_first=True, dropout=dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "    def forward(self, src):                         # src: (B, T)\n",
        "        x = self.dropout(self.embed(src))           # (B, T, E)\n",
        "        outputs, (h, c) = self.lstm(x)              # outputs: (B,T,2H)  h,c: (2*n_layers,B,H)\n",
        "        return outputs, h, c\n",
        "\n",
        "\n",
        "# ================ Bridge: BiLSTM → 4-layer decoder ================\n",
        "class Bridge(nn.Module):\n",
        "    \"\"\"Concat forward/backward (2H) -> project to H, then tile to dec_layers.\"\"\"\n",
        "    def __init__(self, enc_layers=2, dec_layers=4, hid_dim=512):\n",
        "        super().__init__()\n",
        "        self.h_proj = nn.Linear(2*hid_dim, hid_dim)\n",
        "        self.c_proj = nn.Linear(2*hid_dim, hid_dim)\n",
        "        self.dec_layers = dec_layers\n",
        "\n",
        "    def forward(self, h, c):                        # h,c: (2*enc_layers, B, H)\n",
        "        n2, B, H = h.size()\n",
        "        enc_layers = n2 // 2\n",
        "        h = h.view(enc_layers, 2, B, H)\n",
        "        c = c.view(enc_layers, 2, B, H)\n",
        "        h = torch.cat([h[:,0], h[:,1]], dim=-1)     # (enc_layers,B,2H)\n",
        "        c = torch.cat([c[:,0], c[:,1]], dim=-1)     # (enc_layers,B,2H)\n",
        "        h = torch.tanh(self.h_proj(h))              # (enc_layers,B,H)\n",
        "        c = torch.tanh(self.c_proj(c))              # (enc_layers,B,H)\n",
        "        reps = self.dec_layers // enc_layers\n",
        "        rem  = self.dec_layers %  enc_layers\n",
        "        h = h.repeat_interleave(reps, dim=0)\n",
        "        c = c.repeat_interleave(reps, dim=0)\n",
        "        if rem:\n",
        "            h = torch.cat([h, h[-1:].repeat(rem,1,1)], dim=0)\n",
        "            c = torch.cat([c, c[-1:].repeat(rem,1,1)], dim=0)\n",
        "        return h, c                                  # (dec_layers,B,H)\n",
        "\n",
        "\n",
        "# ================ Luong (general) Attention ================\n",
        "class LuongAttention(nn.Module):\n",
        "    def __init__(self, hid_dim):\n",
        "        super().__init__()\n",
        "        self.Wa = nn.Linear(hid_dim, hid_dim, bias=False)\n",
        "\n",
        "    def forward(self, dec_h, enc_outs, enc_mask):\n",
        "        proj = self.Wa(enc_outs)                                 # (B,T,H)\n",
        "        scores = torch.bmm(proj, dec_h.unsqueeze(2)).squeeze(2)  # (B,T)\n",
        "        scores = scores.masked_fill(enc_mask == 0, float('-inf'))\n",
        "        attn = torch.softmax(scores, dim=-1)                     # (B,T)\n",
        "        ctx = torch.bmm(attn.unsqueeze(1), enc_outs).squeeze(1)  # (B,H)\n",
        "        return ctx, attn\n",
        "\n",
        "\n",
        "# ================ Decoder with Attention ================\n",
        "class AttnDecoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim=256, hid_dim=512, n_layers=4, dropout=0.3, pad_idx=0):\n",
        "        super().__init__()\n",
        "        self.embed   = nn.Embedding(output_dim, emb_dim, padding_idx=pad_idx)\n",
        "        self.lstm    = nn.LSTM(emb_dim, hid_dim, num_layers=n_layers, batch_first=True, dropout=dropout)\n",
        "        self.attn    = LuongAttention(hid_dim)\n",
        "        self.concat  = nn.Linear(hid_dim*2, hid_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.n_layers = n_layers\n",
        "        self.hid_dim  = hid_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        # weight tying\n",
        "        if hid_dim == emb_dim:\n",
        "            self.fc_out = nn.Linear(hid_dim, output_dim, bias=False)\n",
        "            self.fc_out.weight = self.embed.weight\n",
        "            self.proj = None\n",
        "        else:\n",
        "            self.proj  = nn.Linear(hid_dim, emb_dim, bias=False)\n",
        "            self.fc_out = nn.Linear(emb_dim, output_dim, bias=False)\n",
        "            self.fc_out.weight = self.embed.weight\n",
        "\n",
        "    def forward(self, input_t, hidden, cell, enc_outs, enc_mask):\n",
        "        emb = self.dropout(self.embed(input_t.unsqueeze(1)))      # (B,1,E)\n",
        "        lstm_out, (hidden, cell) = self.lstm(emb, (hidden, cell)) # (B,1,H)\n",
        "        h_t = lstm_out.squeeze(1)                                 # (B,H)\n",
        "        ctx, _ = self.attn(h_t, enc_outs, enc_mask)               # (B,H)\n",
        "        cat = torch.tanh(self.concat(torch.cat([h_t, ctx], dim=-1)))\n",
        "        if self.proj is not None:\n",
        "            cat = self.proj(cat)\n",
        "        logits = self.fc_out(cat)                                 # (B,V)\n",
        "        return logits, hidden, cell\n",
        "\n",
        "\n",
        "# ================ Seq2Seq wrapper (uses Bridge + Attention) ================\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, pad_idx=0, bos_idx=1, eos_idx=2, device=None):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.pad_idx, self.bos_idx, self.eos_idx = pad_idx, bos_idx, eos_idx\n",
        "        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.bridge = Bridge(enc_layers=encoder.n_layers, dec_layers=decoder.n_layers, hid_dim=encoder.hid_dim)\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "        return (src != self.pad_idx).long()\n",
        "\n",
        "    def forward(self, src, trg=None, teacher_forcing_ratio=0.5, max_len=None):\n",
        "        B = src.size(0)\n",
        "        T_out = (trg.size(1) if trg is not None else max_len)\n",
        "        assert T_out is not None, \"Provide trg or max_len\"\n",
        "\n",
        "        enc_outs2H, h, c = self.encoder(src)        # (B,T,2H)\n",
        "        Bsz, Tsrc, H2 = enc_outs2H.size()\n",
        "        H = H2 // 2\n",
        "        enc_outs = enc_outs2H.view(Bsz, Tsrc, 2, H).sum(dim=2)    # (B,T,H)\n",
        "\n",
        "        h, c = self.bridge(h, c)                    # (n_dec,B,H)\n",
        "        src_mask = self.make_src_mask(src)          # (B,T)\n",
        "\n",
        "        V = self.decoder.output_dim\n",
        "        outputs = torch.zeros(B, T_out, V, device=self.device, dtype=torch.float)\n",
        "\n",
        "        input_t = (trg[:, 0] if trg is not None else\n",
        "                   torch.full((B,), self.bos_idx, dtype=torch.long, device=self.device))\n",
        "\n",
        "        hidden, cell = h, c\n",
        "        for t in range(1, T_out):\n",
        "            logits, hidden, cell = self.decoder(input_t, hidden, cell, enc_outs, src_mask)\n",
        "            outputs[:, t, :] = logits\n",
        "            use_tf = (trg is not None) and (torch.rand(1).item() < teacher_forcing_ratio)\n",
        "            input_t = trg[:, t] if use_tf else logits.argmax(dim=-1)\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "vlf7fZhqPayk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "class LabelSmoothingCE(nn.Module):\n",
        "    def __init__(self, eps=0.1, ignore_index=0):\n",
        "        super().__init__()\n",
        "        self.eps = eps; self.ignore_index = ignore_index\n",
        "    def forward(self, logits, target):\n",
        "        V = logits.size(-1)\n",
        "        logp = torch.log_softmax(logits, dim=-1)\n",
        "        mask = target.ne(self.ignore_index)\n",
        "        if mask.sum() == 0: return logits.new_tensor(0.)\n",
        "        logp = logp[mask]; target = target[mask]\n",
        "        nll = -logp.gather(1, target.unsqueeze(1)).squeeze(1)\n",
        "        smooth = -logp.mean(dim=1)\n",
        "        return ((1 - self.eps) * nll + self.eps * smooth).mean()\n",
        "\n",
        "@torch.no_grad()\n",
        "def token_accuracy(logits, target, ignore_index=0):\n",
        "    preds = logits.argmax(dim=-1)\n",
        "    mask = target.ne(ignore_index)\n",
        "    if mask.sum() == 0: return 0.0\n",
        "    return ((preds == target) & mask).sum().float().item() / mask.sum().float().item()\n"
      ],
      "metadata": {
        "id": "GkjM0MRwPhZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Training & Hyperparameters\n"
      ],
      "metadata": {
        "id": "PHI4qb10rf0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "def train_epoch(model, loader, optimizer, criterion, teacher_forcing=0.5, clip=1.0):\n",
        "    model.train(); total_loss=0.0; total_acc=0.0\n",
        "    for src,trg in loader:\n",
        "        src, trg = src.to(model.device), trg.to(model.device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(src, trg, teacher_forcing_ratio=teacher_forcing)\n",
        "        loss = criterion(logits[:,1:].reshape(-1, logits.size(-1)), trg[:,1:].reshape(-1))\n",
        "        loss.backward(); nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        total_acc  += token_accuracy(logits[:,1:], trg[:,1:])\n",
        "    n = max(1, len(loader))\n",
        "    return total_loss/n, total_acc/n\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, criterion):\n",
        "    model.eval(); total_loss=0.0; total_acc=0.0\n",
        "    for src,trg in loader:\n",
        "        src, trg = src.to(model.device), trg.to(model.device)\n",
        "        logits = model(src, trg, teacher_forcing_ratio=0.0)\n",
        "        loss = criterion(logits[:,1:].reshape(-1, logits.size(-1)), trg[:,1:].reshape(-1))\n",
        "        total_loss += loss.item()\n",
        "        total_acc  += token_accuracy(logits[:,1:], trg[:,1:])\n",
        "    n = max(1, len(loader))\n",
        "    avg = total_loss/n\n",
        "    ppl = float(torch.exp(torch.tensor(avg)))\n",
        "    return avg, ppl, total_acc/n\n"
      ],
      "metadata": {
        "id": "iQOFFMSjPiir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build dataloaders (if not built)\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "BATCH_SIZE = 64\n",
        "train_dl = DataLoader(TensorDataset(train_X, train_Y), batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dl   = DataLoader(TensorDataset(val_X,   val_Y),   batch_size=BATCH_SIZE)\n",
        "\n",
        "# build model\n",
        "pad, bos, eos = 0, 1, 2\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "enc = Encoder(ur_sp.get_piece_size(), emb_dim=256, hid_dim=512, n_layers=2, dropout=0.3, pad_idx=pad)\n",
        "dec = AttnDecoder(ro_sp.get_piece_size(), emb_dim=256, hid_dim=512, n_layers=4, dropout=0.3, pad_idx=pad)  # <- attention decoder\n",
        "\n",
        "model = Seq2Seq(enc, dec, pad_idx=pad, bos_idx=bos, eos_idx=eos, device=device).to(device)\n",
        "\n",
        "print(\"decoder class:\", type(model.decoder))  # should show AttnDecoder\n",
        "\n",
        "# quick forward check\n",
        "x = train_X[:8].to(device)\n",
        "y = train_Y[:8].to(device)\n",
        "with torch.no_grad():\n",
        "    logits = model(x, y, teacher_forcing_ratio=0.0)\n",
        "print(\"OK →\", logits.shape)  # expect (8, T, V)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vZ4OcbqPlXG",
        "outputId": "4e658696-f5f8-45df-9427-1f731cd3af00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "decoder class: <class '__main__.AttnDecoder'>\n",
            "OK → torch.Size([8, 50, 8000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math, torch\n",
        "\n",
        "criterion = LabelSmoothingCE(eps=0.1, ignore_index=0)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-6)\n",
        "\n",
        "best_val = float('inf'); patience, bad = 5, 0\n",
        "for epoch in range(1, 50+1):        # cap at 50; early stopping will cut it earlier\n",
        "    tr_loss, tr_acc = train_epoch(model, train_dl, optimizer, criterion, teacher_forcing=0.5)\n",
        "    val_loss, val_ppl, val_acc = evaluate(model, val_dl, criterion)\n",
        "    print(f\"Epoch {epoch:02d} | train {tr_loss:.4f} acc {tr_acc:.2%} | \"\n",
        "          f\"val {val_loss:.4f} acc {val_acc:.2%} | ppl {val_ppl:.2f}\")\n",
        "    # save best\n",
        "    if val_loss < best_val - 1e-4:\n",
        "        best_val, bad = val_loss, 0\n",
        "        torch.save(model.state_dict(), \"best_attn_seq2seq.pt\")\n",
        "    else:\n",
        "        bad += 1\n",
        "        if bad >= patience:\n",
        "            print(\"Early stopping.\")\n",
        "            break\n",
        "\n",
        "# load best before test\n",
        "model.load_state_dict(torch.load(\"best_attn_seq2seq.pt\", map_location=model.device))\n",
        "test_loss, test_ppl, test_acc = evaluate(model, val_dl, criterion)  # or test_dl if you kept it aside\n",
        "print(f\"FINAL | val loss {test_loss:.4f} acc {test_acc:.2%} ppl {test_ppl:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qd9Z680kP3xd",
        "outputId": "082c6e62-d769-48a9-a13b-0b2130fe65aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | train 5.7636 acc 35.54% | val 4.6664 acc 48.11% | ppl 106.31\n",
            "Epoch 02 | train 4.3164 acc 51.50% | val 4.0471 acc 54.97% | ppl 57.23\n",
            "Epoch 03 | train 3.7560 acc 58.34% | val 3.8843 acc 57.18% | ppl 48.63\n",
            "Epoch 04 | train 3.3604 acc 63.60% | val 3.8009 acc 58.37% | ppl 44.74\n",
            "Epoch 05 | train 3.0326 acc 68.56% | val 3.5740 acc 63.90% | ppl 35.66\n",
            "Epoch 06 | train 2.7653 acc 72.93% | val 3.5033 acc 65.58% | ppl 33.23\n",
            "Epoch 07 | train 2.5575 acc 76.08% | val 3.4414 acc 66.03% | ppl 31.23\n",
            "Epoch 08 | train 2.3819 acc 78.98% | val 3.4073 acc 67.34% | ppl 30.18\n",
            "Epoch 09 | train 2.2443 acc 81.27% | val 3.3817 acc 68.12% | ppl 29.42\n",
            "Epoch 10 | train 2.1253 acc 83.72% | val 3.4094 acc 68.60% | ppl 30.25\n",
            "Epoch 11 | train 2.0378 acc 85.45% | val 3.4640 acc 68.22% | ppl 31.94\n",
            "Epoch 12 | train 1.9499 acc 87.20% | val 3.3350 acc 69.47% | ppl 28.08\n",
            "Epoch 13 | train 1.8881 acc 88.66% | val 3.3579 acc 69.53% | ppl 28.73\n",
            "Epoch 14 | train 1.8291 acc 90.05% | val 3.3822 acc 70.06% | ppl 29.44\n",
            "Epoch 15 | train 1.7891 acc 90.79% | val 3.3980 acc 70.05% | ppl 29.91\n",
            "Epoch 16 | train 1.7504 acc 91.78% | val 3.3570 acc 70.68% | ppl 28.70\n",
            "Epoch 17 | train 1.7121 acc 92.78% | val 3.3374 acc 71.05% | ppl 28.15\n",
            "Early stopping.\n",
            "FINAL | val loss 3.3350 acc 69.47% ppl 28.08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install sacrebleu\n",
        "import sacrebleu, torch\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_bleu(model, pairs, ur_sp, ro_sp, max_len=50):\n",
        "    hyps, refs = [], []\n",
        "    for ur, ro in pairs:\n",
        "        hyp = greedy_decode(model, ur_sp, ro_sp, ur, max_len)\n",
        "        hyps.append(hyp); refs.append([ro])\n",
        "    bleu = sacrebleu.corpus_bleu(hyps, list(zip(*refs))).score\n",
        "    chrf = sacrebleu.corpus_chrf(hyps, list(zip(*refs))).score\n",
        "    return bleu, chrf\n"
      ],
      "metadata": {
        "id": "_FEUNxNuXsRB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ab125fc-e629-4c08-af81-4f81658139f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def beam_decode(model, ur_sp, ro_sp, src_text, max_len=50, beam_size=4):\n",
        "    BOS, EOS, PAD = 1,2,0\n",
        "    src = encode_sentences(ur_sp, [src_text], max_len=max_len).to(model.device)\n",
        "    enc_outs2H, h, c = model.encoder(src)\n",
        "    B,T,H2 = enc_outs2H.size(); H=H2//2\n",
        "    enc_outs = enc_outs2H.view(B,T,2,H).sum(2)\n",
        "    h,c = model.bridge(h,c)\n",
        "    src_mask = (src!=PAD).long()\n",
        "\n",
        "    beams = [(0.0, [BOS], h, c)]  # (logprob, ids, h, c)\n",
        "    for _ in range(1, max_len):\n",
        "        new = []\n",
        "        for lp, ids, hh, cc in beams:\n",
        "            if ids[-1]==EOS: new.append((lp, ids, hh, cc)); continue\n",
        "            inp = torch.tensor([ids[-1]], device=model.device)\n",
        "            logits, hh2, cc2 = model.decoder(inp, hh, cc, enc_outs, src_mask)\n",
        "            logp = torch.log_softmax(logits, -1).squeeze(0)  # (V,)\n",
        "            topk = torch.topk(logp, beam_size).indices.tolist()\n",
        "            for tid in topk:\n",
        "                new.append((lp+float(logp[tid]), ids+[tid], hh2, cc2))\n",
        "        beams = sorted(new, key=lambda x: x[0], reverse=True)[:beam_size]\n",
        "        if all(b[1][-1]==EOS for b in beams): break\n",
        "\n",
        "    best = max(beams, key=lambda x: x[0])[1]\n",
        "    toks = [t for t in best if t not in (PAD,BOS,EOS)]\n",
        "    return ro_sp.decode(toks)\n"
      ],
      "metadata": {
        "id": "HnvW5mTiXxKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Testing"
      ],
      "metadata": {
        "id": "8MXatpf1_lAo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exp-A"
      ],
      "metadata": {
        "id": "11OBynRaYCAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# retrain tokenizers\n",
        "(train_X, train_Y), (val_X, val_Y), (test_X, test_Y), ur_sp, ro_sp = preprocess_pipeline(\n",
        "    urdu, roman, vocab_size=8000, max_len=50   # roman vocab smaller\n",
        ")\n",
        "\n",
        "# rebuild dataloaders\n",
        "train_dl = DataLoader(TensorDataset(train_X, train_Y), batch_size=64, shuffle=True)\n",
        "val_dl   = DataLoader(TensorDataset(val_X,   val_Y),   batch_size=64)\n",
        "\n",
        "# rebuild model\n",
        "enc = Encoder(ur_sp.get_piece_size(), emb_dim=256, hid_dim=512, n_layers=2, dropout=0.3, pad_idx=0)\n",
        "dec = AttnDecoder(ro_sp.get_piece_size(), emb_dim=256, hid_dim=512, n_layers=4, dropout=0.3, pad_idx=0)\n",
        "model = Seq2Seq(enc, dec, pad_idx=0, bos_idx=1, eos_idx=2).to(device)\n"
      ],
      "metadata": {
        "id": "MAd9Ss9DYEgz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bb0ab2d-f155-4c78-d69f-b6a21722ef9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Retry] urdu: Internal: src/trainer_interface.cc(664) [(trainer_spec_.vocab_size()) == (model_proto->pieces_size())] Vocabulary size too high (8000). Please set it to a value <= 5927. → trying vocab_size=7500\n",
            "[Retry] urdu: Internal: src/trainer_interface.cc(664) [(trainer_spec_.vocab_size()) == (model_proto->pieces_size())] Vocabulary size too high (7500). Please set it to a value <= 5927. → trying vocab_size=7000\n",
            "[Retry] urdu: Internal: src/trainer_interface.cc(664) [(trainer_spec_.vocab_size()) == (model_proto->pieces_size())] Vocabulary size too high (7000). Please set it to a value <= 5927. → trying vocab_size=6500\n",
            "[Retry] urdu: Internal: src/trainer_interface.cc(664) [(trainer_spec_.vocab_size()) == (model_proto->pieces_size())] Vocabulary size too high (6500). Please set it to a value <= 5927. → trying vocab_size=6000\n",
            "[Retry] urdu: Internal: src/trainer_interface.cc(664) [(trainer_spec_.vocab_size()) == (model_proto->pieces_size())] Vocabulary size too high (6000). Please set it to a value <= 5927. → trying vocab_size=5500\n",
            "[OK] Trained urdu with vocab_size=5500\n",
            "[OK] Trained roman with vocab_size=8000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exp-B"
      ],
      "metadata": {
        "id": "DchUm756YHry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enc = Encoder(ur_sp.get_piece_size(), emb_dim=256, hid_dim=512, n_layers=2, dropout=0.2, pad_idx=0)\n",
        "dec = AttnDecoder(ro_sp.get_piece_size(), emb_dim=256, hid_dim=512, n_layers=4, dropout=0.2, pad_idx=0)\n",
        "model = Seq2Seq(enc, dec, pad_idx=0, bos_idx=1, eos_idx=2).to(device)\n",
        "\n",
        "criterion = LabelSmoothingCE(eps=0.05, ignore_index=0)   # smoother eps\n"
      ],
      "metadata": {
        "id": "gaOPoeVSYKFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exp-C"
      ],
      "metadata": {
        "id": "hmEuAHUXYptf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enc = Encoder(ur_sp.get_piece_size(), emb_dim=512, hid_dim=512, n_layers=2, dropout=0.3, pad_idx=0)\n",
        "dec = AttnDecoder(ro_sp.get_piece_size(), emb_dim=512, hid_dim=512, n_layers=4, dropout=0.3, pad_idx=0)\n",
        "model = Seq2Seq(enc, dec, pad_idx=0, bos_idx=1, eos_idx=2).to(device)\n"
      ],
      "metadata": {
        "id": "zurBL4oCYQFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = LabelSmoothingCE(eps=0.1, ignore_index=0)  # unless changed\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "best_val = float('inf'); bad = 0; patience = 3\n",
        "for epoch in range(1, 8):   # ~5–7 epochs enough for experiments\n",
        "    tr_loss, tr_acc = train_epoch(model, train_dl, optimizer, criterion)\n",
        "    val_loss, val_ppl, val_acc = evaluate(model, val_dl, criterion)\n",
        "    print(f\"Exp | Epoch {epoch:02d} | train {tr_loss:.4f} acc {tr_acc:.2%} \"\n",
        "          f\"| val {val_loss:.4f} acc {val_acc:.2%} | ppl {val_ppl:.2f}\")\n",
        "    if val_loss < best_val - 1e-4:\n",
        "        best_val, bad = val_loss, 0\n",
        "    else:\n",
        "        bad += 1\n",
        "        if bad >= patience:\n",
        "            print(\"Early stopping.\")\n",
        "            break\n"
      ],
      "metadata": {
        "id": "MX7Vnw5oYRaR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c847fed3-0945-4dd5-9f50-0d67294fc10b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exp | Epoch 01 | train 5.3521 acc 36.63% | val 4.3471 acc 47.16% | ppl 77.25\n",
            "Exp | Epoch 02 | train 3.7539 acc 55.33% | val 3.8367 acc 52.98% | ppl 46.37\n",
            "Exp | Epoch 03 | train 3.1836 acc 63.21% | val 3.5159 acc 61.38% | ppl 33.65\n",
            "Exp | Epoch 04 | train 2.7742 acc 69.72% | val 3.4031 acc 64.42% | ppl 30.06\n",
            "Exp | Epoch 05 | train 2.4791 acc 74.93% | val 3.3568 acc 66.67% | ppl 28.70\n",
            "Exp | Epoch 06 | train 2.2654 acc 79.09% | val 3.3463 acc 67.62% | ppl 28.40\n",
            "Exp | Epoch 07 | train 2.1096 acc 82.56% | val 3.2615 acc 68.77% | ppl 26.09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing Section"
      ],
      "metadata": {
        "id": "_sMdNt6p_r5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, random, numpy as np, torch\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed); torch.backends.cudnn.deterministic = True\n",
        "set_seed(42)\n",
        "\n",
        "def cfg_name(prefix, ur_v, ro_v, emb, hid, drop, lr):\n",
        "    return f\"{prefix}_ur{ur_v}_ro{ro_v}_emb{emb}_hid{hid}_drop{drop}_lr{lr}.pt\"\n"
      ],
      "metadata": {
        "id": "2aTXot78wpuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# greedy decode with bounds/EOS checks\n",
        "@torch.no_grad()\n",
        "def greedy_decode(model, ur_sp, ro_sp, src_text, max_len=50):\n",
        "    model.eval()\n",
        "    PAD, BOS, EOS = 0, 1, 2\n",
        "    src = encode_sentences(ur_sp, [src_text], max_len=max_len).to(model.device)\n",
        "    logits = model(src, trg=None, teacher_forcing_ratio=0.0, max_len=max_len)  # (1,T,V)\n",
        "    ids = logits.argmax(dim=-1).squeeze(0).tolist()\n",
        "    V = ro_sp.get_piece_size()\n",
        "    toks = []\n",
        "    for t in ids:\n",
        "        if t == EOS: break\n",
        "        if t in (PAD, BOS): continue\n",
        "        if 0 <= t < V: toks.append(t)\n",
        "    return ro_sp.decode(toks)\n",
        "\n",
        "# simple BLEU/chrF on a small list of (src, tgt) pairs\n",
        "!pip -q install sacrebleu\n",
        "import sacrebleu\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_bleu(model, sample_pairs, ur_sp, ro_sp, max_len=50):\n",
        "    hyps, refs = [], []\n",
        "    for ur, ro in sample_pairs:\n",
        "        hyp = greedy_decode(model, ur_sp, ro_sp, ur, max_len=max_len)\n",
        "        hyps.append(hyp); refs.append([ro])\n",
        "    bleu = sacrebleu.corpus_bleu(hyps, list(zip(*refs))).score\n",
        "    chrf = sacrebleu.corpus_chrf(hyps, list(zip(*refs))).score\n",
        "    return bleu, chrf\n"
      ],
      "metadata": {
        "id": "lK_E7EB94F70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exp-A"
      ],
      "metadata": {
        "id": "L31LqN4b7sIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)\n",
        "\n",
        "# 1) re-run preprocessing with target vocab ≈ 8000\n",
        "(train_X, train_Y), (val_X, val_Y), (test_X, test_Y), ur_sp, ro_sp = preprocess_pipeline(\n",
        "    urdu, roman, vocab_size=8000, max_len=50\n",
        ")\n",
        "\n",
        "# 2) dataloaders\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "BATCH = 64\n",
        "train_dl = DataLoader(TensorDataset(train_X, train_Y), batch_size=BATCH, shuffle=True)\n",
        "val_dl   = DataLoader(TensorDataset(val_X,   val_Y),   batch_size=BATCH)\n",
        "\n",
        "# 3) build model\n",
        "pad, bos, eos = 0, 1, 2\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "URV, ROV = ur_sp.get_piece_size(), ro_sp.get_piece_size()\n",
        "EMB, HID, DROP = 256, 512, 0.3\n",
        "LR = 1e-3\n",
        "\n",
        "enc = Encoder(URV, emb_dim=EMB, hid_dim=HID, n_layers=2, dropout=DROP, pad_idx=pad)\n",
        "dec = AttnDecoder(ROV, emb_dim=EMB, hid_dim=HID, n_layers=4, dropout=DROP, pad_idx=pad)\n",
        "model = Seq2Seq(enc, dec, pad_idx=pad, bos_idx=bos, eos_idx=eos, device=device).to(device)\n",
        "\n",
        "criterion = LabelSmoothingCE(eps=0.10, ignore_index=pad)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-6)\n",
        "\n",
        "# 4) train (short with early stop)\n",
        "best, bad, patience = float('inf'), 0, 3\n",
        "ckpt_A = cfg_name(\"best_attn_A\", URV, ROV, EMB, HID, DROP, LR)\n",
        "for epoch in range(1, 9):\n",
        "    tr_loss, tr_acc = train_epoch(model, train_dl, optimizer, criterion, teacher_forcing=0.5)\n",
        "    val_loss, val_ppl, val_acc = evaluate(model, val_dl, criterion)\n",
        "    print(f\"[A] e{epoch:02d} | train {tr_loss:.4f} acc {tr_acc:.2%} | val {val_loss:.4f} acc {val_acc:.2%} | ppl {val_ppl:.2f}\")\n",
        "    if val_loss < best - 1e-4:\n",
        "        best, bad = val_loss, 0\n",
        "        torch.save(model.state_dict(), ckpt_A)\n",
        "    else:\n",
        "        bad += 1\n",
        "        if bad >= patience:\n",
        "            print(\"[A] early stop\"); break\n",
        "\n",
        "# 5) (optional) small BLEU on 50 val pairs\n",
        "pairs_A = [(ur_sp.decode(train_X[i].tolist()), ro_sp.decode(train_Y[i].tolist())) for i in range(50)]\n",
        "bleu_A, chrf_A = eval_bleu(model, pairs_A, ur_sp, ro_sp, max_len=50)\n",
        "print(f\"[A] BLEU {bleu_A:.2f}  chrF {chrf_A:.2f}\")\n",
        "\n",
        "res_A = dict(exp=\"A\", urv=URV, rov=ROV, emb=EMB, hid=HID, drop=DROP, lr=LR,\n",
        "             best_val=best, bleu=bleu_A, chrf=chrf_A)\n"
      ],
      "metadata": {
        "id": "6jfooUDa4HJY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f33c475-7002-4422-c01d-67bd3abd653f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Retry] urdu: Internal: src/trainer_interface.cc(664) [(trainer_spec_.vocab_size()) == (model_proto->pieces_size())] Vocabulary size too high (8000). Please set it to a value <= 5927. → trying vocab_size=7500\n",
            "[Retry] urdu: Internal: src/trainer_interface.cc(664) [(trainer_spec_.vocab_size()) == (model_proto->pieces_size())] Vocabulary size too high (7500). Please set it to a value <= 5927. → trying vocab_size=7000\n",
            "[Retry] urdu: Internal: src/trainer_interface.cc(664) [(trainer_spec_.vocab_size()) == (model_proto->pieces_size())] Vocabulary size too high (7000). Please set it to a value <= 5927. → trying vocab_size=6500\n",
            "[Retry] urdu: Internal: src/trainer_interface.cc(664) [(trainer_spec_.vocab_size()) == (model_proto->pieces_size())] Vocabulary size too high (6500). Please set it to a value <= 5927. → trying vocab_size=6000\n",
            "[Retry] urdu: Internal: src/trainer_interface.cc(664) [(trainer_spec_.vocab_size()) == (model_proto->pieces_size())] Vocabulary size too high (6000). Please set it to a value <= 5927. → trying vocab_size=5500\n",
            "[OK] Trained urdu with vocab_size=5500\n",
            "[OK] Trained roman with vocab_size=8000\n",
            "[A] e01 | train 5.7910 acc 35.22% | val 4.6507 acc 47.35% | ppl 104.66\n",
            "[A] e02 | train 4.3176 acc 51.28% | val 4.3108 acc 48.76% | ppl 74.50\n",
            "[A] e03 | train 3.7539 acc 58.01% | val 3.8202 acc 59.61% | ppl 45.61\n",
            "[A] e04 | train 3.3624 acc 63.50% | val 3.6267 acc 61.90% | ppl 37.59\n",
            "[A] e05 | train 3.0354 acc 68.62% | val 3.7064 acc 59.95% | ppl 40.71\n",
            "[A] e06 | train 2.7751 acc 72.74% | val 3.6451 acc 63.28% | ppl 38.29\n",
            "[A] e07 | train 2.5696 acc 75.84% | val 3.4466 acc 66.24% | ppl 31.39\n",
            "[A] e08 | train 2.3810 acc 79.16% | val 3.4295 acc 67.16% | ppl 30.86\n",
            "[A] BLEU 74.19  chrF 86.26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exp-B"
      ],
      "metadata": {
        "id": "4htjKCyk7ovS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)\n",
        "\n",
        "# re-use train_dl / val_dl built above (same tokenizers)\n",
        "URV, ROV = ur_sp.get_piece_size(), ro_sp.get_piece_size()\n",
        "EMB, HID, DROP = 256, 512, 0.2\n",
        "LR = 1e-3\n",
        "\n",
        "enc = Encoder(URV, emb_dim=EMB, hid_dim=HID, n_layers=2, dropout=DROP, pad_idx=0)\n",
        "dec = AttnDecoder(ROV, emb_dim=EMB, hid_dim=HID, n_layers=4, dropout=DROP, pad_idx=0)\n",
        "model = Seq2Seq(enc, dec, pad_idx=0, bos_idx=1, eos_idx=2, device=device).to(device)\n",
        "\n",
        "criterion = LabelSmoothingCE(eps=0.05, ignore_index=0)  # milder smoothing\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-6)\n",
        "\n",
        "best, bad, patience = float('inf'), 0, 3\n",
        "ckpt_B = cfg_name(\"best_attn_B\", URV, ROV, EMB, HID, DROP, LR)\n",
        "for epoch in range(1, 9):\n",
        "    tr_loss, tr_acc = train_epoch(model, train_dl, optimizer, criterion, teacher_forcing=0.5)\n",
        "    val_loss, val_ppl, val_acc = evaluate(model, val_dl, criterion)\n",
        "    print(f\"[B] e{epoch:02d} | train {tr_loss:.4f} acc {tr_acc:.2%} | val {val_loss:.4f} acc {val_acc:.2%} | ppl {val_ppl:.2f}\")\n",
        "    if val_loss < best - 1e-4:\n",
        "        best, bad = val_loss, 0\n",
        "        torch.save(model.state_dict(), ckpt_B)\n",
        "    else:\n",
        "        bad += 1\n",
        "        if bad >= patience:\n",
        "            print(\"[B] early stop\"); break\n",
        "\n",
        "pairs_B = [(ur_sp.decode(train_X[i].tolist()), ro_sp.decode(train_Y[i].tolist())) for i in range(50)]\n",
        "bleu_B, chrf_B = eval_bleu(model, pairs_B, ur_sp, ro_sp, max_len=50)\n",
        "print(f\"[B] BLEU {bleu_B:.2f}  chrF {chrf_B:.2f}\")\n",
        "\n",
        "res_B = dict(exp=\"B\", urv=URV, rov=ROV, emb=EMB, hid=HID, drop=DROP, lr=LR,\n",
        "             best_val=best, bleu=bleu_B, chrf=chrf_B)\n"
      ],
      "metadata": {
        "id": "TsB8v-sh4KU0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "066502a5-e5e9-41ed-bf8c-b5355bda81eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[B] e01 | train 5.3976 acc 36.57% | val 4.1496 acc 49.61% | ppl 63.41\n",
            "[B] e02 | train 3.7446 acc 53.07% | val 3.8512 acc 50.99% | ppl 47.05\n",
            "[B] e03 | train 3.0888 acc 60.40% | val 3.3281 acc 60.33% | ppl 27.88\n",
            "[B] e04 | train 2.6026 acc 66.94% | val 3.1171 acc 62.69% | ppl 22.58\n",
            "[B] e05 | train 2.2139 acc 72.65% | val 3.0836 acc 64.50% | ppl 21.84\n",
            "[B] e06 | train 1.9093 acc 77.30% | val 3.1003 acc 65.41% | ppl 22.20\n",
            "[B] e07 | train 1.6749 acc 80.87% | val 3.0585 acc 66.04% | ppl 21.30\n",
            "[B] e08 | train 1.4694 acc 84.74% | val 3.0984 acc 66.54% | ppl 22.16\n",
            "[B] BLEU 81.59  chrF 89.64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exp-C"
      ],
      "metadata": {
        "id": "Ptk-R2TI7yNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)\n",
        "\n",
        "URV, ROV = ur_sp.get_piece_size(), ro_sp.get_piece_size()\n",
        "EMB, HID, DROP = 512, 512, 0.3\n",
        "LR = 5e-4  # slightly lower LR for bigger emb\n",
        "\n",
        "enc = Encoder(URV, emb_dim=EMB, hid_dim=HID, n_layers=2, dropout=DROP, pad_idx=0)\n",
        "dec = AttnDecoder(ROV, emb_dim=EMB, hid_dim=HID, n_layers=4, dropout=DROP, pad_idx=0)\n",
        "model = Seq2Seq(enc, dec, pad_idx=0, bos_idx=1, eos_idx=2, device=device).to(device)\n",
        "\n",
        "criterion = LabelSmoothingCE(eps=0.10, ignore_index=0)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-6)\n",
        "\n",
        "best, bad, patience = float('inf'), 0, 3\n",
        "ckpt_C = cfg_name(\"best_attn_C\", URV, ROV, EMB, HID, DROP, LR)\n",
        "for epoch in range(1, 9):\n",
        "    tr_loss, tr_acc = train_epoch(model, train_dl, optimizer, criterion, teacher_forcing=0.5)\n",
        "    val_loss, val_ppl, val_acc = evaluate(model, val_dl, criterion)\n",
        "    print(f\"[C] e{epoch:02d} | train {tr_loss:.4f} acc {tr_acc:.2%} | val {val_loss:.4f} acc {val_acc:.2%} | ppl {val_ppl:.2f}\")\n",
        "    if val_loss < best - 1e-4:\n",
        "        best, bad = val_loss, 0\n",
        "        torch.save(model.state_dict(), ckpt_C)\n",
        "    else:\n",
        "        bad += 1\n",
        "        if bad >= patience:\n",
        "            print(\"[C] early stop\"); break\n",
        "\n",
        "pairs_C = [(ur_sp.decode(train_X[i].tolist()), ro_sp.decode(train_Y[i].tolist())) for i in range(50)]\n",
        "bleu_C, chrf_C = eval_bleu(model, pairs_C, ur_sp, ro_sp, max_len=50)\n",
        "print(f\"[C] BLEU {bleu_C:.2f}  chrF {chrf_C:.2f}\")\n",
        "\n",
        "res_C = dict(exp=\"C\", urv=URV, rov=ROV, emb=EMB, hid=HID, drop=DROP, lr=LR,\n",
        "             best_val=best, bleu=bleu_C, chrf=chrf_C)\n"
      ],
      "metadata": {
        "id": "Fly0vFTx4Sso",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "276a3463-0a84-4753-c269-771305f8bc59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[C] e01 | train 5.5824 acc 35.37% | val 4.4607 acc 48.34% | ppl 86.55\n",
            "[C] e02 | train 3.9710 acc 53.70% | val 3.8195 acc 56.79% | ppl 45.58\n",
            "[C] e03 | train 3.4276 acc 59.92% | val 3.5926 acc 59.09% | ppl 36.33\n",
            "[C] e04 | train 3.0499 acc 65.15% | val 3.5070 acc 62.26% | ppl 33.35\n",
            "[C] e05 | train 2.7586 acc 69.74% | val 3.3704 acc 64.37% | ppl 29.09\n",
            "[C] e06 | train 2.5397 acc 73.29% | val 3.3972 acc 64.73% | ppl 29.88\n",
            "[C] e07 | train 2.3426 acc 77.14% | val 3.3181 acc 66.81% | ppl 27.61\n",
            "[C] e08 | train 2.1844 acc 80.38% | val 3.3695 acc 66.64% | ppl 29.06\n",
            "[C] BLEU 75.80  chrF 86.36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Results"
      ],
      "metadata": {
        "id": "gFCabouq_w3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas import DataFrame\n",
        "results = [res_A, res_B, res_C]\n",
        "df = DataFrame(results, columns=[\"exp\",\"urv\",\"rov\",\"emb\",\"hid\",\"drop\",\"lr\",\"best_val\",\"bleu\",\"chrf\"])\n",
        "display(df)\n",
        "print(\"\\nMarkdown table for your report:\\n\")\n",
        "print(df.to_markdown(index=False))\n"
      ],
      "metadata": {
        "id": "TAo5pxbX4WL3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "85fc3c37-b4f9-4f8e-bc07-2133cac1ce99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  exp   urv   rov  emb  hid  drop      lr  best_val       bleu       chrf\n",
              "0   A  5500  8000  256  512   0.3  0.0010  3.429504  74.186658  86.258480\n",
              "1   B  5500  8000  256  512   0.2  0.0010  3.058476  81.588795  89.642438\n",
              "2   C  5500  8000  512  512   0.3  0.0005  3.318058  75.795069  86.356333"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c5b6a1cd-a6ae-43b8-bb2d-46ffbba53f8d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>exp</th>\n",
              "      <th>urv</th>\n",
              "      <th>rov</th>\n",
              "      <th>emb</th>\n",
              "      <th>hid</th>\n",
              "      <th>drop</th>\n",
              "      <th>lr</th>\n",
              "      <th>best_val</th>\n",
              "      <th>bleu</th>\n",
              "      <th>chrf</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A</td>\n",
              "      <td>5500</td>\n",
              "      <td>8000</td>\n",
              "      <td>256</td>\n",
              "      <td>512</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>3.429504</td>\n",
              "      <td>74.186658</td>\n",
              "      <td>86.258480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>B</td>\n",
              "      <td>5500</td>\n",
              "      <td>8000</td>\n",
              "      <td>256</td>\n",
              "      <td>512</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>3.058476</td>\n",
              "      <td>81.588795</td>\n",
              "      <td>89.642438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>C</td>\n",
              "      <td>5500</td>\n",
              "      <td>8000</td>\n",
              "      <td>512</td>\n",
              "      <td>512</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>3.318058</td>\n",
              "      <td>75.795069</td>\n",
              "      <td>86.356333</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c5b6a1cd-a6ae-43b8-bb2d-46ffbba53f8d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c5b6a1cd-a6ae-43b8-bb2d-46ffbba53f8d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c5b6a1cd-a6ae-43b8-bb2d-46ffbba53f8d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-68fe31a3-1698-49ec-99ef-550c10835b07\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-68fe31a3-1698-49ec-99ef-550c10835b07')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-68fe31a3-1698-49ec-99ef-550c10835b07 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_f7812ec6-9802-444d-a1e1-3eeb9d366a03\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_f7812ec6-9802-444d-a1e1-3eeb9d366a03 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"exp\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"A\",\n          \"B\",\n          \"C\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"urv\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 5500,\n        \"max\": 5500,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          5500\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rov\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 8000,\n        \"max\": 8000,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          8000\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"emb\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 147,\n        \"min\": 256,\n        \"max\": 512,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          512\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hid\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 512,\n        \"max\": 512,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          512\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"drop\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05773502691896256,\n        \"min\": 0.2,\n        \"max\": 0.3,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lr\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0002886751345948129,\n        \"min\": 0.0005,\n        \"max\": 0.001,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0005\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"best_val\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.19037874271470215,\n        \"min\": 3.058476341776101,\n        \"max\": 3.4295038746063966,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          3.4295038746063966\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bleu\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.893282208924818,\n        \"min\": 74.1866578193256,\n        \"max\": 81.588794748406,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          74.1866578193256\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chrf\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.9261025252372803,\n        \"min\": 86.25848037535548,\n        \"max\": 89.64243772270709,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          86.25848037535548\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Markdown table for your report:\n",
            "\n",
            "| exp   |   urv |   rov |   emb |   hid |   drop |     lr |   best_val |    bleu |    chrf |\n",
            "|:------|------:|------:|------:|------:|-------:|-------:|-----------:|--------:|--------:|\n",
            "| A     |  5500 |  8000 |   256 |   512 |    0.3 | 0.001  |    3.4295  | 74.1867 | 86.2585 |\n",
            "| B     |  5500 |  8000 |   256 |   512 |    0.2 | 0.001  |    3.05848 | 81.5888 | 89.6424 |\n",
            "| C     |  5500 |  8000 |   512 |   512 |    0.3 | 0.0005 |    3.31806 | 75.7951 | 86.3563 |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "samples = [\n",
        "    # \"مجھے اردو بہت پسند ہے\"\n",
        "   \"وقت کا کیا ہے گزرتا ہے گزر جائے گا\"\n",
        "    # \"ہر مشکل کے بعد آسانی ضرور آتی ہے۔\"\n",
        "\n",
        "]\n",
        "\n",
        "for s in samples:\n",
        "    hyp_greedy = greedy_decode(model, ur_sp, ro_sp, s, max_len=50)\n",
        "    print(f\"SRC: {s}\\nROMAN: {hyp_greedy}\\n\")\n"
      ],
      "metadata": {
        "id": "UY0r3wP0ASXQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d5f253c-5bbc-4416-ff01-dec360095ef0"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SRC: وقت کا کیا ہے گزرتا ہے گزر جائے گا\n",
            "ROMAN: vaqt k ky hai guzart hai guzar jgg\n",
            "\n"
          ]
        }
      ]
    }
  ]
}